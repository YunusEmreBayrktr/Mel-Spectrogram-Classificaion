{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4567c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.curdir = \"C:\\\\Users\\\\bayra\\\\Desktop\\\\Courses\\\\AIN311\\\\assignment3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16e37fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample-084720.png</td>\n",
       "      <td>i had seen all that it would presently bring me</td>\n",
       "      <td>twenties</td>\n",
       "      <td>other</td>\n",
       "      <td>england</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample-169346.png</td>\n",
       "      <td>a friend had told the boy about the shop and h...</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>england</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample-027740.png</td>\n",
       "      <td>the boy said nothing</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample-035454.png</td>\n",
       "      <td>what is the matter</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample-134062.png</td>\n",
       "      <td>no baselines or comparison to state of the art...</td>\n",
       "      <td>twenties</td>\n",
       "      <td>female</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename                                               text  \\\n",
       "0  sample-084720.png    i had seen all that it would presently bring me   \n",
       "1  sample-169346.png  a friend had told the boy about the shop and h...   \n",
       "2  sample-027740.png                               the boy said nothing   \n",
       "3  sample-035454.png                                 what is the matter   \n",
       "4  sample-134062.png  no baselines or comparison to state of the art...   \n",
       "\n",
       "        age  gender     accent  \n",
       "0  twenties   other    england  \n",
       "1  twenties    male    england  \n",
       "2  twenties    male     indian  \n",
       "3  twenties    male         us  \n",
       "4  twenties  female  australia  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading provided csv files\n",
    "train = pd.read_csv(\"train_data.csv\")\n",
    "test = pd.read_csv(\"test_data.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb705ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample-084720.png</td>\n",
       "      <td>twenties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample-169346.png</td>\n",
       "      <td>twenties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample-027740.png</td>\n",
       "      <td>twenties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample-035454.png</td>\n",
       "      <td>twenties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample-134062.png</td>\n",
       "      <td>twenties</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename       age\n",
       "0  sample-084720.png  twenties\n",
       "1  sample-169346.png  twenties\n",
       "2  sample-027740.png  twenties\n",
       "3  sample-035454.png  twenties\n",
       "4  sample-134062.png  twenties"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping text, gender and accent columns.\n",
    "train.drop(columns=[\"text\", \"gender\", \"accent\"], inplace=True)\n",
    "test.drop(columns=[\"text\", \"gender\", \"accent\"], inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eebaec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample-084720.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample-169346.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample-027740.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample-035454.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample-134062.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  age\n",
       "0  sample-084720.png    1\n",
       "1  sample-169346.png    1\n",
       "2  sample-027740.png    1\n",
       "3  sample-035454.png    1\n",
       "4  sample-134062.png    1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing age column with integers.\n",
    "replace = {\"teens\"   : 0,\n",
    "           \"twenties\": 1,\n",
    "           \"thirties\": 2,\n",
    "           \"fourties\": 3,\n",
    "           \"fifties\" : 4,\n",
    "           \"sixties\" : 5,\n",
    "}\n",
    "\n",
    "train.replace(replace, inplace=True)\n",
    "test.replace(replace, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5a9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images.\n",
    "def load_images(df, file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        image_path = os.curdir + \"\\\\\" + file + \"\\\\\" + row['filename']\n",
    "        img = Image.open(image_path).convert(\"L\")\n",
    "        img = img.resize((100, 100))\n",
    "        img_array = np.array(img) \n",
    "        X.append(img_array/255.0)\n",
    "        Y.append(row['age'])\n",
    "        del img, img_array\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b059d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading images.\n",
    "trainX, trainY = load_images(train, \"train\")\n",
    "testX, testY = load_images(test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36145a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting labels to one hot encoding.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "trainY = one_hot_encoder.fit_transform(np.array(trainY).reshape(-1, 1))\n",
    "testY = one_hot_encoder.fit_transform(np.array(testY).reshape(-1, 1))\n",
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11de0837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening trainX and testX.\n",
    "trainX_flt = trainX.reshape((9600,10000))\n",
    "testX_flt = testX.reshape((2400,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6443cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network class for MLP.\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        self.weights, self.biases = self.initialize_parameters()\n",
    "\n",
    "    # To initialize weight randomly at the beginning\n",
    "    def initialize_parameters(self):\n",
    "        sizes = [self.input_size] + self.hidden_layers + [self.output_size]\n",
    "        weights = [np.random.randn(sizes[i], sizes[i+1]) for i in range(len(sizes)-1)]\n",
    "        biases = [np.zeros((1, sizes[i+1])) for i in range(len(sizes)-1)]\n",
    "        return weights, biases\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def activate(self, x):\n",
    "        activation_functions = {\n",
    "            'sigmoid': self.sigmoid,\n",
    "            'softmax': self.softmax,\n",
    "        }\n",
    "        return activation_functions[self.activation](x)\n",
    "\n",
    "    # Function to perform forward propagation\n",
    "    def forward_propagation(self, x):\n",
    "        activations = [x]\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            a = self.activate(z)\n",
    "            activations.append(a)\n",
    "        \n",
    "        output = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        output = self.activate(output)\n",
    "        activations.append(output)\n",
    "        \n",
    "        return activations\n",
    "\n",
    "    # Funtion to calc the sum of the negative log-likelihood of the correct labels.\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 0.00000001)) / y_true.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    # Funtion to perform back propagation\n",
    "    def backward_propagation(self, x, y_true, activations):\n",
    "        m = x.shape[0]\n",
    "        gradients = [activations[-1] - y_true]\n",
    "        \n",
    "        # Backpropagation for hidden layers\n",
    "        for i in range(len(self.hidden_layers), 0, -1):\n",
    "            dz = np.dot(gradients[0], self.weights[i].T)\n",
    "            da = dz * (activations[i] > 0) if self.activation == 'relu' else dz\n",
    "            dw = np.dot(activations[i-1].T, da) / m\n",
    "            db = np.sum(da, axis=0, keepdims=True) / m\n",
    "            gradients.insert(0, da)\n",
    "            self.weights[i-1] -= self.learning_rate * dw\n",
    "            self.biases[i-1] -= self.learning_rate * db\n",
    "        \n",
    "        # Backpropagation for the input layer\n",
    "        dw = np.dot(x.T, gradients[0]) / m\n",
    "        db = np.sum(gradients[0], axis=0, keepdims=True) / m\n",
    "        self.weights[0] -= self.learning_rate * dw\n",
    "        self.biases[0] -= self.learning_rate * db\n",
    "\n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        for epoch in range(epochs):\n",
    "            activations = self.forward_propagation(x)\n",
    "            loss = self.compute_loss(y, activations[-1])\n",
    "            self.backward_propagation(x, y, activations)\n",
    "            \n",
    "            if (epoch%5) == 0:\n",
    "                accuracy = self.compute_accuracy(y, activations[-1])\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}, Accuracy: {accuracy}%\")\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        true_labels = np.argmax(y_true, axis=1)\n",
    "        correct_predictions = np.sum(predictions == true_labels)\n",
    "        total_examples = y_true.shape[0]\n",
    "        accuracy = (correct_predictions / total_examples)\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, x):\n",
    "        activations = self.forward_propagation(x)\n",
    "        return activations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3074428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dict to store the results.\n",
    "results = {}\n",
    "results[\"Model\"] = []\n",
    "results[\"Input Size\"] = []\n",
    "results[\"Activation Func\"] = []\n",
    "results[\"Hidden Layer Size\"] = []\n",
    "results[\"Learnin Rate\"] = []\n",
    "results[\"Accuracy\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443568b",
   "metadata": {},
   "source": [
    "# MLP with 0 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61bad211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with activation: 'sigmoid' || learning rate: '0.005'\n",
      "Epoch 0, Loss: 8.136536952259085, Accuracy: 0.16260416666666666%\n",
      "Epoch 5, Loss: 9.44068591564693, Accuracy: 0.159375%\n",
      "Epoch 10, Loss: 9.952744342897653, Accuracy: 0.161875%\n",
      "Epoch 15, Loss: 10.007350231879945, Accuracy: 0.1615625%\n",
      "Test Accuracy with activation: 'sigmoid' || learning rate: '0.005' = 0.15791666666666668\n",
      "\n",
      "Experiment with activation: 'sigmoid' || learning rate: '0.01'\n",
      "Epoch 0, Loss: 10.206851243003316, Accuracy: 0.16677083333333334%\n",
      "Epoch 5, Loss: 10.664782528113356, Accuracy: 0.164375%\n",
      "Epoch 10, Loss: 10.525892672994495, Accuracy: 0.15958333333333333%\n",
      "Epoch 15, Loss: 9.995340469600178, Accuracy: 0.16302083333333334%\n",
      "Test Accuracy with activation: 'sigmoid' || learning rate: '0.01' = 0.165\n",
      "\n",
      "Experiment with activation: 'sigmoid' || learning rate: '0.017'\n",
      "Epoch 0, Loss: 10.310798805818004, Accuracy: 0.16427083333333334%\n",
      "Epoch 5, Loss: 10.754555141543019, Accuracy: 0.15427083333333333%\n",
      "Epoch 10, Loss: 10.215916431778602, Accuracy: 0.15635416666666666%\n",
      "Epoch 15, Loss: 10.064738959215587, Accuracy: 0.156875%\n",
      "Test Accuracy with activation: 'sigmoid' || learning rate: '0.017' = 0.15875\n",
      "\n",
      "Experiment with activation: 'softmax' || learning rate: '0.005'\n",
      "Epoch 0, Loss: 12.85131407484393, Accuracy: 0.17375%\n",
      "Epoch 5, Loss: 12.523749067831655, Accuracy: 0.17041666666666666%\n",
      "Epoch 10, Loss: 12.432504826578851, Accuracy: 0.17010416666666667%\n",
      "Epoch 15, Loss: 12.40265322653056, Accuracy: 0.16802083333333334%\n",
      "Test Accuracy with activation: 'softmax' || learning rate: '0.005' = 0.18125\n",
      "\n",
      "Experiment with activation: 'softmax' || learning rate: '0.01'\n",
      "Epoch 0, Loss: 14.492002864265821, Accuracy: 0.16177083333333334%\n",
      "Epoch 5, Loss: 13.840491567922458, Accuracy: 0.16260416666666666%\n",
      "Epoch 10, Loss: 12.862079062020475, Accuracy: 0.16458333333333333%\n",
      "Epoch 15, Loss: 12.184944512470544, Accuracy: 0.16541666666666666%\n",
      "Test Accuracy with activation: 'softmax' || learning rate: '0.01' = 0.16083333333333333\n",
      "\n",
      "Experiment with activation: 'softmax' || learning rate: '0.017'\n",
      "Epoch 0, Loss: 15.105785489433769, Accuracy: 0.160625%\n",
      "Epoch 5, Loss: 11.837810604082625, Accuracy: 0.15625%\n",
      "Epoch 10, Loss: 11.01305895011029, Accuracy: 0.16416666666666666%\n",
      "Epoch 15, Loss: 10.929923644156558, Accuracy: 0.16583333333333333%\n",
      "Test Accuracy with activation: 'softmax' || learning rate: '0.017' = 0.15958333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activations = ['sigmoid', 'softmax']\n",
    "learnings   = [0.005, 0.010, 0.017]\n",
    "layer = []\n",
    "for activation in activations:\n",
    "    for rate in learnings:\n",
    "        print(f\"Experiment with activation: '{activation}' || learning rate: '{rate}'\")\n",
    "        model = NeuralNetwork(10000, layer, 6, activation)\n",
    "        model.train(trainX_flt, trainY, epochs=20, learning_rate=rate)\n",
    "        accuracy = model.compute_accuracy(testY, model.predict(testX_flt))\n",
    "        print(f\"Test Accuracy with activation: '{activation}' || learning rate: '{rate}' = {accuracy}\\n\")\n",
    "        results[\"Model\"].append(\"MLP\")\n",
    "        results[\"Input Size\"].append(\"100x100x1\")\n",
    "        results[\"Activation Func\"].append(activation)\n",
    "        results[\"Hidden Layer Size\"].append(layer)\n",
    "        results[\"Learnin Rate\"].append(rate)\n",
    "        results[\"Accuracy\"].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45767c53",
   "metadata": {},
   "source": [
    "# MLP with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f745e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with activation:'sigmoid' || learning rate:'0.005' || layer:'[64]'\n",
      "Epoch 0, Loss: 2.6475761880565125, Accuracy: 0.16322916666666668%\n",
      "Epoch 5, Loss: 2.859680422740019, Accuracy: 0.16260416666666666%\n",
      "Epoch 10, Loss: 2.8093831974751358, Accuracy: 0.16489583333333332%\n",
      "Epoch 15, Loss: 2.7947526236246256, Accuracy: 0.16572916666666668%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.005' || layer:'[64]' = 0.165\n",
      "\n",
      "Experiment with activation:'sigmoid' || learning rate:'0.01' || layer:'[64]'\n",
      "Epoch 0, Loss: 3.4554129946279053, Accuracy: 0.16458333333333333%\n",
      "Epoch 5, Loss: 3.1609959920964585, Accuracy: 0.165%\n",
      "Epoch 10, Loss: 2.999537673186368, Accuracy: 0.16875%\n",
      "Epoch 15, Loss: 3.6118239241075436, Accuracy: 0.16520833333333335%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.01' || layer:'[64]' = 0.15958333333333333\n",
      "\n",
      "Experiment with activation:'sigmoid' || learning rate:'0.017' || layer:'[64]'\n",
      "Epoch 0, Loss: 5.440965406610985, Accuracy: 0.16770833333333332%\n",
      "Epoch 5, Loss: 3.376587851860604, Accuracy: 0.17072916666666665%\n",
      "Epoch 10, Loss: 4.204786495065097, Accuracy: 0.16645833333333335%\n",
      "Epoch 15, Loss: 4.505523024294806, Accuracy: 0.17333333333333334%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.017' || layer:'[64]' = 0.17291666666666666\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.005' || layer:'[64]'\n",
      "Epoch 0, Loss: 2.0133971190282534, Accuracy: 0.16114583333333332%\n",
      "Epoch 5, Loss: 2.0126679857763734, Accuracy: 0.16135416666666666%\n",
      "Epoch 10, Loss: 2.013491567072175, Accuracy: 0.1646875%\n",
      "Epoch 15, Loss: 2.013004809813127, Accuracy: 0.163125%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.005' || layer:'[64]' = 0.15125\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.01' || layer:'[64]'\n",
      "Epoch 0, Loss: 2.0392939763934343, Accuracy: 0.16895833333333332%\n",
      "Epoch 5, Loss: 2.068437007120018, Accuracy: 0.171875%\n",
      "Epoch 10, Loss: 2.0612698877185833, Accuracy: 0.1740625%\n",
      "Epoch 15, Loss: 2.0615213438032587, Accuracy: 0.176875%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.01' || layer:'[64]' = 0.17458333333333334\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.017' || layer:'[64]'\n",
      "Epoch 0, Loss: 2.240765081714973, Accuracy: 0.16614583333333333%\n",
      "Epoch 5, Loss: 2.236461183530613, Accuracy: 0.16614583333333333%\n",
      "Epoch 10, Loss: 2.2073355871617264, Accuracy: 0.164375%\n",
      "Epoch 15, Loss: 2.246248208016979, Accuracy: 0.1696875%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.017' || layer:'[64]' = 0.15958333333333333\n",
      "\n",
      "Experiment with activation:'sigmoid' || learning rate:'0.005' || layer:'[128]'\n",
      "Epoch 0, Loss: 1.3405118660549256, Accuracy: 0.17270833333333332%\n",
      "Epoch 5, Loss: 3.294784194450791, Accuracy: 0.17375%\n",
      "Epoch 10, Loss: 3.2740363099135146, Accuracy: 0.17322916666666666%\n",
      "Epoch 15, Loss: 3.2441080165263, Accuracy: 0.1734375%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.005' || layer:'[128]' = 0.1725\n",
      "\n",
      "Experiment with activation:'sigmoid' || learning rate:'0.01' || layer:'[128]'\n",
      "Epoch 0, Loss: 4.9979340078494445, Accuracy: 0.16208333333333333%\n",
      "Epoch 5, Loss: 3.728374544932219, Accuracy: 0.1859375%\n",
      "Epoch 10, Loss: 5.249669827955222, Accuracy: 0.178125%\n",
      "Epoch 15, Loss: 6.028012348503498, Accuracy: 0.1828125%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.01' || layer:'[128]' = 0.18208333333333335\n",
      "\n",
      "Experiment with activation:'sigmoid' || learning rate:'0.017' || layer:'[128]'\n",
      "Epoch 0, Loss: 2.810711495472264, Accuracy: 0.16802083333333334%\n",
      "Epoch 5, Loss: 8.547201133479787, Accuracy: 0.1653125%\n",
      "Epoch 10, Loss: 10.321965514205528, Accuracy: 0.16041666666666668%\n",
      "Epoch 15, Loss: 9.531315266210328, Accuracy: 0.16635416666666666%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.017' || layer:'[128]' = 0.16708333333333333\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.005' || layer:'[128]'\n",
      "Epoch 0, Loss: 2.1151110963924893, Accuracy: 0.16822916666666668%\n",
      "Epoch 5, Loss: 2.0804183675242767, Accuracy: 0.16625%\n",
      "Epoch 10, Loss: 2.068802362103714, Accuracy: 0.16510416666666666%\n",
      "Epoch 15, Loss: 2.065240194127255, Accuracy: 0.1640625%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.005' || layer:'[128]' = 0.17958333333333334\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.01' || layer:'[128]'\n",
      "Epoch 0, Loss: 2.4315265403541133, Accuracy: 0.1659375%\n",
      "Epoch 5, Loss: 2.2558882243940754, Accuracy: 0.17302083333333335%\n",
      "Epoch 10, Loss: 2.27350299905128, Accuracy: 0.17260416666666667%\n",
      "Epoch 15, Loss: 2.2613766964296884, Accuracy: 0.17625%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.01' || layer:'[128]' = 0.17\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.017' || layer:'[128]'\n",
      "Epoch 0, Loss: 1.9989165391040122, Accuracy: 0.15552083333333333%\n",
      "Epoch 5, Loss: 2.256986472932029, Accuracy: 0.150625%\n",
      "Epoch 10, Loss: 2.0145689376622116, Accuracy: 0.15875%\n",
      "Epoch 15, Loss: 2.1551101395733783, Accuracy: 0.15625%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.017' || layer:'[128]' = 0.16583333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activations = ['sigmoid', 'softmax']\n",
    "learnings   = [0.005, 0.01, 0.017]\n",
    "\n",
    "for layer in [[64],[128]]:\n",
    "    for activation in activations:\n",
    "        for rate in learnings:\n",
    "            print(f\"Experiment with activation:'{activation}' || learning rate:'{rate}' || layer:'{layer}'\")\n",
    "            model = NeuralNetwork(10000, layer, 6, activation)\n",
    "            model.train(trainX_flt, trainY, epochs=20, learning_rate=rate)\n",
    "            accuracy = model.compute_accuracy(testY, model.predict(testX_flt))\n",
    "            print(f\"Test Accuracy with activation:'{activation}' || learning rate:'{rate}' || layer:'{layer}' = {accuracy}\\n\")\n",
    "            results[\"Model\"].append(\"MLP\")\n",
    "            results[\"Input Size\"].append(\"100x100x1\")\n",
    "            results[\"Activation Func\"].append(activation)\n",
    "            results[\"Hidden Layer Size\"].append(layer)\n",
    "            results[\"Learnin Rate\"].append(rate)\n",
    "            results[\"Accuracy\"].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85329eff",
   "metadata": {},
   "source": [
    "# MLP with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f142eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with activation:'sigmoid' || learning rate:'0.005' || layer:'[128, 64]'\n",
      "Epoch 0, Loss: 2.5811813254531444, Accuracy: 0.16979166666666667%\n",
      "Epoch 5, Loss: 5.989723539627912, Accuracy: 0.16583333333333333%\n",
      "Epoch 10, Loss: 6.999226453479582, Accuracy: 0.17125%\n",
      "Epoch 15, Loss: 4.903949771739373, Accuracy: 0.1675%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.005' || layer:'[128, 64]' = 0.16625\n",
      "\n",
      "Experiment with activation:'sigmoid' || learning rate:'0.01' || layer:'[128, 64]'\n",
      "Epoch 0, Loss: 1.951566218307645, Accuracy: 0.16895833333333332%\n",
      "Epoch 5, Loss: 3.1656636776875735, Accuracy: 0.16697916666666668%\n",
      "Epoch 10, Loss: 2.7330048264393807, Accuracy: 0.16666666666666666%\n",
      "Epoch 15, Loss: 2.3150065299630778, Accuracy: 0.17114583333333333%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.01' || layer:'[128, 64]' = 0.18541666666666667\n",
      "\n",
      "Experiment with activation:'sigmoid' || learning rate:'0.017' || layer:'[128, 64]'\n",
      "Epoch 0, Loss: 1.2699727874770157, Accuracy: 0.1678125%\n",
      "Epoch 5, Loss: 2.7826913655773082, Accuracy: 0.1665625%\n",
      "Epoch 10, Loss: 3.290966226641924, Accuracy: 0.16666666666666666%\n",
      "Epoch 15, Loss: 2.768695850459361, Accuracy: 0.18208333333333335%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.017' || layer:'[128, 64]' = 0.16458333333333333\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.005' || layer:'[128, 64]'\n",
      "Epoch 0, Loss: 1.7940851367198338, Accuracy: 0.18416666666666667%\n",
      "Epoch 5, Loss: 1.79229402335707, Accuracy: 0.175625%\n",
      "Epoch 10, Loss: 1.7951442452900714, Accuracy: 0.17583333333333334%\n",
      "Epoch 15, Loss: 1.795371698492813, Accuracy: 0.17895833333333333%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.005' || layer:'[128, 64]' = 0.185\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.01' || layer:'[128, 64]'\n",
      "Epoch 0, Loss: 1.8108093228735838, Accuracy: 0.17177083333333334%\n",
      "Epoch 5, Loss: 1.7967983775972012, Accuracy: 0.1640625%\n",
      "Epoch 10, Loss: 1.7942327277300634, Accuracy: 0.16489583333333332%\n",
      "Epoch 15, Loss: 1.7927752229415073, Accuracy: 0.16697916666666668%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.01' || layer:'[128, 64]' = 0.16625\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.017' || layer:'[128, 64]'\n",
      "Epoch 0, Loss: 1.809250276314223, Accuracy: 0.15875%\n",
      "Epoch 5, Loss: 1.7942074733629234, Accuracy: 0.173125%\n",
      "Epoch 10, Loss: 1.7927342446137653, Accuracy: 0.18166666666666667%\n",
      "Epoch 15, Loss: 1.7926176367161788, Accuracy: 0.18197916666666666%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.017' || layer:'[128, 64]' = 0.1825\n",
      "\n",
      "Experiment with activation:'sigmoid' || learning rate:'0.005' || layer:'[256, 128]'\n",
      "Epoch 0, Loss: 2.4512440780639775, Accuracy: 0.16666666666666666%\n",
      "Epoch 5, Loss: 3.9554307209427453, Accuracy: 0.15875%\n",
      "Epoch 10, Loss: 5.674225365007157, Accuracy: 0.16895833333333332%\n",
      "Epoch 15, Loss: 3.6911838416495666, Accuracy: 0.17083333333333334%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.005' || layer:'[256, 128]' = 0.17666666666666667\n",
      "\n",
      "Experiment with activation:'sigmoid' || learning rate:'0.01' || layer:'[256, 128]'\n",
      "Epoch 0, Loss: 4.871144497800106, Accuracy: 0.15614583333333334%\n",
      "Epoch 5, Loss: 9.21930049456089, Accuracy: 0.165625%\n",
      "Epoch 10, Loss: 11.312943172022738, Accuracy: 0.16875%\n",
      "Epoch 15, Loss: 11.071126572310606, Accuracy: 0.16802083333333334%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.01' || layer:'[256, 128]' = 0.17\n",
      "\n",
      "Experiment with activation:'sigmoid' || learning rate:'0.017' || layer:'[256, 128]'\n",
      "Epoch 0, Loss: 2.3670858416570106, Accuracy: 0.1603125%\n",
      "Epoch 5, Loss: 6.432621279436108, Accuracy: 0.16697916666666668%\n",
      "Epoch 10, Loss: 4.43180318969658, Accuracy: 0.16666666666666666%\n",
      "Epoch 15, Loss: 3.1112112230416544, Accuracy: 0.1690625%\n",
      "Test Accuracy with activation:'sigmoid' || learning rate:'0.017' || layer:'[256, 128]' = 0.16666666666666666\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.005' || layer:'[256, 128]'\n",
      "Epoch 0, Loss: 1.7987216222466058, Accuracy: 0.16260416666666666%\n",
      "Epoch 5, Loss: 1.7961756927597146, Accuracy: 0.16635416666666666%\n",
      "Epoch 10, Loss: 1.79438887981549, Accuracy: 0.17177083333333334%\n",
      "Epoch 15, Loss: 1.7942215261981778, Accuracy: 0.17625%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.005' || layer:'[256, 128]' = 0.17791666666666667\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.01' || layer:'[256, 128]'\n",
      "Epoch 0, Loss: 1.8003432535160175, Accuracy: 0.168125%\n",
      "Epoch 5, Loss: 1.793024423725241, Accuracy: 0.17520833333333333%\n",
      "Epoch 10, Loss: 1.793399479123292, Accuracy: 0.17322916666666666%\n",
      "Epoch 15, Loss: 1.7924993179710038, Accuracy: 0.176875%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.01' || layer:'[256, 128]' = 0.17041666666666666\n",
      "\n",
      "Experiment with activation:'softmax' || learning rate:'0.017' || layer:'[256, 128]'\n",
      "Epoch 0, Loss: 1.8035179113752746, Accuracy: 0.1665625%\n",
      "Epoch 5, Loss: 1.793312827164491, Accuracy: 0.17041666666666666%\n",
      "Epoch 10, Loss: 1.7937239049540514, Accuracy: 0.1665625%\n",
      "Epoch 15, Loss: 1.793345686332547, Accuracy: 0.164375%\n",
      "Test Accuracy with activation:'softmax' || learning rate:'0.017' || layer:'[256, 128]' = 0.16666666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activations = ['sigmoid', 'softmax']\n",
    "learnings   = [0.005, 0.01, 0.017]\n",
    "\n",
    "for layer in [[128,64],[256,128]]:\n",
    "    for activation in activations:\n",
    "        for rate in learnings:\n",
    "            print(f\"Experiment with activation:'{activation}' || learning rate:'{rate}' || layer:'{layer}'\")\n",
    "            model = NeuralNetwork(10000, layer, 6, activation)\n",
    "            model.train(trainX_flt, trainY, epochs=20, learning_rate=rate)\n",
    "            accuracy = model.compute_accuracy(testY, model.predict(testX_flt))\n",
    "            print(f\"Test Accuracy with activation:'{activation}' || learning rate:'{rate}' || layer:'{layer}' = {accuracy}\\n\")\n",
    "            results[\"Model\"].append(\"MLP\")\n",
    "            results[\"Input Size\"].append(\"100x100x1\")\n",
    "            results[\"Activation Func\"].append(activation)\n",
    "            results[\"Hidden Layer Size\"].append(layer)\n",
    "            results[\"Learnin Rate\"].append(rate)\n",
    "            results[\"Accuracy\"].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e32f7",
   "metadata": {},
   "source": [
    "# CNN with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e05e388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "241afc45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with activation:'relu' || layer:'64'\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.7897 - accuracy: 0.1790\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 8s 57ms/step - loss: 1.7791 - accuracy: 0.2145\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 8s 56ms/step - loss: 1.7656 - accuracy: 0.2300\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.7547 - accuracy: 0.2485\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.7430 - accuracy: 0.2586\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.7285 - accuracy: 0.2705\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.7126 - accuracy: 0.2791\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 9s 59ms/step - loss: 1.6976 - accuracy: 0.2926\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.6851 - accuracy: 0.3023\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.6708 - accuracy: 0.3135\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.6575 - accuracy: 0.3151\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 1.6513 - accuracy: 0.3234\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 9s 59ms/step - loss: 1.6397 - accuracy: 0.3279\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.6330 - accuracy: 0.3336\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.6182 - accuracy: 0.3429\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 9s 58ms/step - loss: 1.6014 - accuracy: 0.3521\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 9s 58ms/step - loss: 1.5997 - accuracy: 0.3514\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 8s 56ms/step - loss: 1.5844 - accuracy: 0.3614\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.5734 - accuracy: 0.3663\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 9s 57ms/step - loss: 1.5583 - accuracy: 0.3689\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 1.6223 - accuracy: 0.3275\n",
      "Test Accuracy with activation:'relu' || layer:'64' = 0.32749998569488525\n",
      "\n",
      "Experiment with activation:'sigmoid' || layer:'64'\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.8050 - accuracy: 0.1705\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 10s 67ms/step - loss: 1.7942 - accuracy: 0.1654\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 1.7935 - accuracy: 0.1707\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.7933 - accuracy: 0.1665\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 1.7933 - accuracy: 0.1625\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.7929 - accuracy: 0.1695\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.7932 - accuracy: 0.1624\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 1.7927 - accuracy: 0.1699\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 1.7927 - accuracy: 0.1650\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.7934 - accuracy: 0.1592\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.7929 - accuracy: 0.1625\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.7930 - accuracy: 0.1595\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 1.7928 - accuracy: 0.1682\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.7931 - accuracy: 0.1586\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 10s 67ms/step - loss: 1.7926 - accuracy: 0.1703\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 10s 64ms/step - loss: 1.7932 - accuracy: 0.1629\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 9s 62ms/step - loss: 1.7929 - accuracy: 0.1644\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 9s 61ms/step - loss: 1.7932 - accuracy: 0.1615\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 10s 64ms/step - loss: 1.7929 - accuracy: 0.1616\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 1.7923 - accuracy: 0.1742\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 1.7929 - accuracy: 0.1667\n",
      "Test Accuracy with activation:'sigmoid' || layer:'64' = 0.1666666716337204\n",
      "\n",
      "Experiment with activation:'relu' || layer:'128'\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 10s 63ms/step - loss: 1.7868 - accuracy: 0.1925\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 1.7712 - accuracy: 0.2289\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 1.7542 - accuracy: 0.2532\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 10s 66ms/step - loss: 1.7391 - accuracy: 0.2658\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 10s 63ms/step - loss: 1.7214 - accuracy: 0.2778\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 1.7075 - accuracy: 0.2907\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 1.6887 - accuracy: 0.2995\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 1.6783 - accuracy: 0.3096\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 10s 63ms/step - loss: 1.6582 - accuracy: 0.3237\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 10s 63ms/step - loss: 1.6531 - accuracy: 0.3186\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 1.6318 - accuracy: 0.3355\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 10s 65ms/step - loss: 1.6173 - accuracy: 0.3422\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 10s 63ms/step - loss: 1.6056 - accuracy: 0.3546\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 10s 66ms/step - loss: 1.5970 - accuracy: 0.3546\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 10s 64ms/step - loss: 1.5819 - accuracy: 0.3605\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 9s 63ms/step - loss: 1.5632 - accuracy: 0.3742\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 10s 63ms/step - loss: 1.5572 - accuracy: 0.3764\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 10s 66ms/step - loss: 1.5464 - accuracy: 0.3814\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 10s 67ms/step - loss: 1.5295 - accuracy: 0.3913\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 10s 70ms/step - loss: 1.5164 - accuracy: 0.4009\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 1.6402 - accuracy: 0.3333\n",
      "Test Accuracy with activation:'relu' || layer:'128' = 0.3333333432674408\n",
      "\n",
      "Experiment with activation:'sigmoid' || layer:'128'\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 11s 70ms/step - loss: 1.8161 - accuracy: 0.1672\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 11s 70ms/step - loss: 1.7981 - accuracy: 0.1671\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 10s 68ms/step - loss: 1.7955 - accuracy: 0.1579\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 10s 68ms/step - loss: 1.7950 - accuracy: 0.1702\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 10s 69ms/step - loss: 1.7945 - accuracy: 0.1699\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 10s 70ms/step - loss: 1.7943 - accuracy: 0.1650\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 10s 69ms/step - loss: 1.7936 - accuracy: 0.1692\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 10s 69ms/step - loss: 1.7947 - accuracy: 0.1643\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 11s 71ms/step - loss: 1.7944 - accuracy: 0.1579\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 10s 69ms/step - loss: 1.7948 - accuracy: 0.1631\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 10s 69ms/step - loss: 1.7943 - accuracy: 0.1648\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 10s 70ms/step - loss: 1.7943 - accuracy: 0.1664\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 10s 70ms/step - loss: 1.7940 - accuracy: 0.1726\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 10s 68ms/step - loss: 1.7945 - accuracy: 0.1695\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 10s 69ms/step - loss: 1.7947 - accuracy: 0.1661\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 10s 69ms/step - loss: 1.7942 - accuracy: 0.1608\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 10s 68ms/step - loss: 1.7938 - accuracy: 0.1620\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 10s 68ms/step - loss: 1.7949 - accuracy: 0.1631\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 10s 70ms/step - loss: 1.7940 - accuracy: 0.1693\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 10s 69ms/step - loss: 1.7944 - accuracy: 0.1589\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 1.7953 - accuracy: 0.1667\n",
      "Test Accuracy with activation:'sigmoid' || layer:'128' = 0.1666666716337204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activations = ['relu','sigmoid']\n",
    "for layer in [64,128]:\n",
    "    for activation in activations:\n",
    "            print(f\"Experiment with activation:'{activation}' || layer:'{layer}'\")\n",
    "            model = Sequential([\n",
    "    \n",
    "                    Conv2D(filters=16, kernel_size=(3, 3), activation=activation, input_shape=(100, 100, 1)),\n",
    "                    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "                    Flatten(),\n",
    "                    Dense(layer, activation=activation),\n",
    "                    Dense(6, activation=\"softmax\")\n",
    "                ])\n",
    "            \n",
    "            model.compile(optimizer=SGD(lr=0.01), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "            model.fit(trainX, trainY, batch_size=64, epochs=20)\n",
    "            loss, accuracy = model.evaluate(testX, testY)\n",
    "            print(f\"Test Accuracy with activation:'{activation}' || layer:'{layer}' = {accuracy}\\n\")\n",
    "            results[\"Model\"].append(\"CNN-1-CONV\")\n",
    "            results[\"Input Size\"].append(\"100x100x1\")\n",
    "            results[\"Activation Func\"].append(activation)\n",
    "            results[\"Hidden Layer Size\"].append(layer)\n",
    "            results[\"Learnin Rate\"].append(0.01)\n",
    "            results[\"Accuracy\"].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71714e8",
   "metadata": {},
   "source": [
    "# CNN with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d6775a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with activation:'relu' || layer:'[64, 32]'\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.7908 - accuracy: 0.1807\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 16s 109ms/step - loss: 1.7856 - accuracy: 0.2026\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.7806 - accuracy: 0.2174\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.7722 - accuracy: 0.2258\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 16s 105ms/step - loss: 1.7640 - accuracy: 0.2389\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.7558 - accuracy: 0.2450\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 15s 102ms/step - loss: 1.7462 - accuracy: 0.2596\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 15s 103ms/step - loss: 1.7384 - accuracy: 0.2655\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.7269 - accuracy: 0.2704\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.7137 - accuracy: 0.2846\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.7041 - accuracy: 0.2898\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.6958 - accuracy: 0.2903\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.6816 - accuracy: 0.3017\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.6797 - accuracy: 0.3011\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.6682 - accuracy: 0.3079\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 16s 108ms/step - loss: 1.6621 - accuracy: 0.3117\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 16s 106ms/step - loss: 1.6491 - accuracy: 0.3203\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 16s 110ms/step - loss: 1.6371 - accuracy: 0.3253\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 1.6365 - accuracy: 0.3296\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 16s 108ms/step - loss: 1.6198 - accuracy: 0.3388\n",
      "75/75 [==============================] - 1s 13ms/step - loss: 1.6323 - accuracy: 0.3304\n",
      "Test Accuracy with activation:'relu' || layer:'[64, 32]' = 0.3304166793823242\n",
      "\n",
      "Experiment with activation:'sigmoid' || layer:'[64, 32]'\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 17s 112ms/step - loss: 1.8013 - accuracy: 0.1645\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7929 - accuracy: 0.1611\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7928 - accuracy: 0.1601\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7930 - accuracy: 0.1632\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7928 - accuracy: 0.1594\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7929 - accuracy: 0.1616\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7927 - accuracy: 0.1554\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7928 - accuracy: 0.1624\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 18s 117ms/step - loss: 1.7925 - accuracy: 0.1655\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7927 - accuracy: 0.1636\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7926 - accuracy: 0.1602\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7928 - accuracy: 0.1643\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7926 - accuracy: 0.1617\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7926 - accuracy: 0.1593\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7924 - accuracy: 0.1643\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7924 - accuracy: 0.1679\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 17s 116ms/step - loss: 1.7924 - accuracy: 0.1602\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7924 - accuracy: 0.1648\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7925 - accuracy: 0.1557\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.7924 - accuracy: 0.1648\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 1.7919 - accuracy: 0.1667\n",
      "Test Accuracy with activation:'sigmoid' || layer:'[64, 32]' = 0.1666666716337204\n",
      "\n",
      "Experiment with activation:'relu' || layer:'[128, 64]'\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 17s 109ms/step - loss: 1.7884 - accuracy: 0.1859\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 16s 110ms/step - loss: 1.7787 - accuracy: 0.2106\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 16s 110ms/step - loss: 1.7701 - accuracy: 0.2205\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 17s 110ms/step - loss: 1.7614 - accuracy: 0.2371\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 17s 110ms/step - loss: 1.7525 - accuracy: 0.2473\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 17s 110ms/step - loss: 1.7429 - accuracy: 0.2524\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 16s 110ms/step - loss: 1.7318 - accuracy: 0.2644\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 17s 113ms/step - loss: 1.7219 - accuracy: 0.2764\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 17s 114ms/step - loss: 1.7104 - accuracy: 0.2795\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 17s 111ms/step - loss: 1.7014 - accuracy: 0.2900\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 17s 111ms/step - loss: 1.6905 - accuracy: 0.3047\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 17s 110ms/step - loss: 1.6755 - accuracy: 0.3082\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 17s 111ms/step - loss: 1.6686 - accuracy: 0.3123\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 17s 112ms/step - loss: 1.6467 - accuracy: 0.3241\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 1.6447 - accuracy: 0.3293\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 17s 111ms/step - loss: 1.6290 - accuracy: 0.3357\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 17s 111ms/step - loss: 1.6216 - accuracy: 0.3382\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 17s 111ms/step - loss: 1.6120 - accuracy: 0.3468\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 17s 112ms/step - loss: 1.5925 - accuracy: 0.3556\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 17s 111ms/step - loss: 1.5833 - accuracy: 0.3569\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 1.6163 - accuracy: 0.3329\n",
      "Test Accuracy with activation:'relu' || layer:'[128, 64]' = 0.3329166769981384\n",
      "\n",
      "Experiment with activation:'sigmoid' || layer:'[128, 64]'\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 18s 117ms/step - loss: 1.8035 - accuracy: 0.1659\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 18s 118ms/step - loss: 1.7941 - accuracy: 0.1603\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 18s 117ms/step - loss: 1.7936 - accuracy: 0.1652\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 18s 119ms/step - loss: 1.7941 - accuracy: 0.1561\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 18s 118ms/step - loss: 1.7936 - accuracy: 0.1650\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 18s 118ms/step - loss: 1.7940 - accuracy: 0.1634\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 18s 118ms/step - loss: 1.7936 - accuracy: 0.1669\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 18s 119ms/step - loss: 1.7937 - accuracy: 0.1606\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 18s 118ms/step - loss: 1.7934 - accuracy: 0.1686\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 18s 118ms/step - loss: 1.7935 - accuracy: 0.1624\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 18s 117ms/step - loss: 1.7936 - accuracy: 0.1627\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 18s 117ms/step - loss: 1.7938 - accuracy: 0.1582\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 18s 117ms/step - loss: 1.7934 - accuracy: 0.1659\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 18s 118ms/step - loss: 1.7932 - accuracy: 0.1668\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 18s 117ms/step - loss: 1.7932 - accuracy: 0.1624\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 18s 117ms/step - loss: 1.7930 - accuracy: 0.1628\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 18s 120ms/step - loss: 1.7931 - accuracy: 0.1669\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 18s 117ms/step - loss: 1.7928 - accuracy: 0.1693\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 18s 118ms/step - loss: 1.7932 - accuracy: 0.1670\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 18s 118ms/step - loss: 1.7933 - accuracy: 0.1686\n",
      "75/75 [==============================] - 1s 14ms/step - loss: 1.7928 - accuracy: 0.1667\n",
      "Test Accuracy with activation:'sigmoid' || layer:'[128, 64]' = 0.1666666716337204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activations = ['relu', 'sigmoid']\n",
    "for layer in [64,128]:\n",
    "    for activation in activations:\n",
    "            print(f\"Experiment with activation:'{activation}' || layer:'{[layer, layer//2]}'\")\n",
    "            model = Sequential([\n",
    "    \n",
    "                    Conv2D(filters=16, kernel_size=(3, 3), activation=activation, input_shape=(100, 100, 1)),\n",
    "                    MaxPooling2D(pool_size=(2, 2)),\n",
    "                    \n",
    "                    Conv2D(filters=32, kernel_size=(3, 3), activation=activation),\n",
    "                    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "                    Flatten(),\n",
    "                    Dense(layer, activation=activation),\n",
    "                    Dense((layer//2), activation=activation),\n",
    "                    Dense(6, activation=\"softmax\")\n",
    "                ])\n",
    "            \n",
    "            model.compile(optimizer=SGD(lr=0.01), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "            model.fit(trainX, trainY, batch_size=64, epochs=20)\n",
    "            loss, accuracy = model.evaluate(testX, testY)\n",
    "            print(f\"Test Accuracy with activation:'{activation}' || layer:'{[layer, layer//2]}' = {accuracy}\\n\")\n",
    "            results[\"Model\"].append(\"CNN-2-CONV\")\n",
    "            results[\"Input Size\"].append(\"100x100x1\")\n",
    "            results[\"Activation Func\"].append(activation)\n",
    "            results[\"Hidden Layer Size\"].append(layer)\n",
    "            results[\"Learnin Rate\"].append(0.01)\n",
    "            results[\"Accuracy\"].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd450c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95b54523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input Size</th>\n",
       "      <th>Activation Func</th>\n",
       "      <th>Hidden Layer Size</th>\n",
       "      <th>Learnin Rate</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.157917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.158750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.181250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.160833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.159583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.159583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.172917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.174583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.159583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.182083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.167083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.179583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.165833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.166250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.185417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.164583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.166250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.176667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.177917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.170417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>CNN-1-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>CNN-1-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CNN-1-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>relu</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>CNN-1-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>CNN-2-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.330417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>CNN-2-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>CNN-2-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>relu</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.332917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>CNN-2-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model Input Size Activation Func Hidden Layer Size  Learnin Rate  \\\n",
       "0          MLP  100x100x1         sigmoid                []         0.005   \n",
       "1          MLP  100x100x1         sigmoid                []         0.010   \n",
       "2          MLP  100x100x1         sigmoid                []         0.017   \n",
       "3          MLP  100x100x1         softmax                []         0.005   \n",
       "4          MLP  100x100x1         softmax                []         0.010   \n",
       "5          MLP  100x100x1         softmax                []         0.017   \n",
       "6          MLP  100x100x1         sigmoid              [64]         0.005   \n",
       "7          MLP  100x100x1         sigmoid              [64]         0.010   \n",
       "8          MLP  100x100x1         sigmoid              [64]         0.017   \n",
       "9          MLP  100x100x1         softmax              [64]         0.005   \n",
       "10         MLP  100x100x1         softmax              [64]         0.010   \n",
       "11         MLP  100x100x1         softmax              [64]         0.017   \n",
       "12         MLP  100x100x1         sigmoid             [128]         0.005   \n",
       "13         MLP  100x100x1         sigmoid             [128]         0.010   \n",
       "14         MLP  100x100x1         sigmoid             [128]         0.017   \n",
       "15         MLP  100x100x1         softmax             [128]         0.005   \n",
       "16         MLP  100x100x1         softmax             [128]         0.010   \n",
       "17         MLP  100x100x1         softmax             [128]         0.017   \n",
       "18         MLP  100x100x1         sigmoid         [128, 64]         0.005   \n",
       "19         MLP  100x100x1         sigmoid         [128, 64]         0.010   \n",
       "20         MLP  100x100x1         sigmoid         [128, 64]         0.017   \n",
       "21         MLP  100x100x1         softmax         [128, 64]         0.005   \n",
       "22         MLP  100x100x1         softmax         [128, 64]         0.010   \n",
       "23         MLP  100x100x1         softmax         [128, 64]         0.017   \n",
       "24         MLP  100x100x1         sigmoid        [256, 128]         0.005   \n",
       "25         MLP  100x100x1         sigmoid        [256, 128]         0.010   \n",
       "26         MLP  100x100x1         sigmoid        [256, 128]         0.017   \n",
       "27         MLP  100x100x1         softmax        [256, 128]         0.005   \n",
       "28         MLP  100x100x1         softmax        [256, 128]         0.010   \n",
       "29         MLP  100x100x1         softmax        [256, 128]         0.017   \n",
       "30  CNN-1-CONV  100x100x1            relu                64         0.010   \n",
       "31  CNN-1-CONV  100x100x1         sigmoid                64         0.010   \n",
       "32  CNN-1-CONV  100x100x1            relu               128         0.010   \n",
       "33  CNN-1-CONV  100x100x1         sigmoid               128         0.010   \n",
       "34  CNN-2-CONV  100x100x1            relu                64         0.010   \n",
       "35  CNN-2-CONV  100x100x1         sigmoid                64         0.010   \n",
       "36  CNN-2-CONV  100x100x1            relu               128         0.010   \n",
       "37  CNN-2-CONV  100x100x1         sigmoid               128         0.010   \n",
       "\n",
       "    Accuracy  \n",
       "0   0.157917  \n",
       "1   0.165000  \n",
       "2   0.158750  \n",
       "3   0.181250  \n",
       "4   0.160833  \n",
       "5   0.159583  \n",
       "6   0.165000  \n",
       "7   0.159583  \n",
       "8   0.172917  \n",
       "9   0.151250  \n",
       "10  0.174583  \n",
       "11  0.159583  \n",
       "12  0.172500  \n",
       "13  0.182083  \n",
       "14  0.167083  \n",
       "15  0.179583  \n",
       "16  0.170000  \n",
       "17  0.165833  \n",
       "18  0.166250  \n",
       "19  0.185417  \n",
       "20  0.164583  \n",
       "21  0.185000  \n",
       "22  0.166250  \n",
       "23  0.182500  \n",
       "24  0.176667  \n",
       "25  0.170000  \n",
       "26  0.166667  \n",
       "27  0.177917  \n",
       "28  0.170417  \n",
       "29  0.166667  \n",
       "30  0.327500  \n",
       "31  0.166667  \n",
       "32  0.333333  \n",
       "33  0.166667  \n",
       "34  0.330417  \n",
       "35  0.166667  \n",
       "36  0.332917  \n",
       "37  0.166667  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results)\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b132a",
   "metadata": {},
   "source": [
    "Here we can see our results. To be able to compare them we will sort them by accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7024dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input Size</th>\n",
       "      <th>Activation Func</th>\n",
       "      <th>Hidden Layer Size</th>\n",
       "      <th>Learnin Rate</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CNN-1-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>relu</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>CNN-2-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>relu</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.332917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>CNN-2-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.330417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>CNN-1-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.185417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.182083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.181250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.179583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.177917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.176667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.174583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.172917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.170417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.167083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>CNN-1-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>CNN-1-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>CNN-2-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>CNN-2-CONV</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.166250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.166250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[128]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.165833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.164583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.160833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.159583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.159583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.159583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.158750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.157917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP</td>\n",
       "      <td>100x100x1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>[64]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model Input Size Activation Func Hidden Layer Size  Learnin Rate  \\\n",
       "32  CNN-1-CONV  100x100x1            relu               128         0.010   \n",
       "36  CNN-2-CONV  100x100x1            relu               128         0.010   \n",
       "34  CNN-2-CONV  100x100x1            relu                64         0.010   \n",
       "30  CNN-1-CONV  100x100x1            relu                64         0.010   \n",
       "19         MLP  100x100x1         sigmoid         [128, 64]         0.010   \n",
       "21         MLP  100x100x1         softmax         [128, 64]         0.005   \n",
       "23         MLP  100x100x1         softmax         [128, 64]         0.017   \n",
       "13         MLP  100x100x1         sigmoid             [128]         0.010   \n",
       "3          MLP  100x100x1         softmax                []         0.005   \n",
       "15         MLP  100x100x1         softmax             [128]         0.005   \n",
       "27         MLP  100x100x1         softmax        [256, 128]         0.005   \n",
       "24         MLP  100x100x1         sigmoid        [256, 128]         0.005   \n",
       "10         MLP  100x100x1         softmax              [64]         0.010   \n",
       "8          MLP  100x100x1         sigmoid              [64]         0.017   \n",
       "12         MLP  100x100x1         sigmoid             [128]         0.005   \n",
       "28         MLP  100x100x1         softmax        [256, 128]         0.010   \n",
       "16         MLP  100x100x1         softmax             [128]         0.010   \n",
       "25         MLP  100x100x1         sigmoid        [256, 128]         0.010   \n",
       "14         MLP  100x100x1         sigmoid             [128]         0.017   \n",
       "31  CNN-1-CONV  100x100x1         sigmoid                64         0.010   \n",
       "33  CNN-1-CONV  100x100x1         sigmoid               128         0.010   \n",
       "35  CNN-2-CONV  100x100x1         sigmoid                64         0.010   \n",
       "37  CNN-2-CONV  100x100x1         sigmoid               128         0.010   \n",
       "26         MLP  100x100x1         sigmoid        [256, 128]         0.017   \n",
       "29         MLP  100x100x1         softmax        [256, 128]         0.017   \n",
       "22         MLP  100x100x1         softmax         [128, 64]         0.010   \n",
       "18         MLP  100x100x1         sigmoid         [128, 64]         0.005   \n",
       "17         MLP  100x100x1         softmax             [128]         0.017   \n",
       "1          MLP  100x100x1         sigmoid                []         0.010   \n",
       "6          MLP  100x100x1         sigmoid              [64]         0.005   \n",
       "20         MLP  100x100x1         sigmoid         [128, 64]         0.017   \n",
       "4          MLP  100x100x1         softmax                []         0.010   \n",
       "11         MLP  100x100x1         softmax              [64]         0.017   \n",
       "7          MLP  100x100x1         sigmoid              [64]         0.010   \n",
       "5          MLP  100x100x1         softmax                []         0.017   \n",
       "2          MLP  100x100x1         sigmoid                []         0.017   \n",
       "0          MLP  100x100x1         sigmoid                []         0.005   \n",
       "9          MLP  100x100x1         softmax              [64]         0.005   \n",
       "\n",
       "    Accuracy  \n",
       "32  0.333333  \n",
       "36  0.332917  \n",
       "34  0.330417  \n",
       "30  0.327500  \n",
       "19  0.185417  \n",
       "21  0.185000  \n",
       "23  0.182500  \n",
       "13  0.182083  \n",
       "3   0.181250  \n",
       "15  0.179583  \n",
       "27  0.177917  \n",
       "24  0.176667  \n",
       "10  0.174583  \n",
       "8   0.172917  \n",
       "12  0.172500  \n",
       "28  0.170417  \n",
       "16  0.170000  \n",
       "25  0.170000  \n",
       "14  0.167083  \n",
       "31  0.166667  \n",
       "33  0.166667  \n",
       "35  0.166667  \n",
       "37  0.166667  \n",
       "26  0.166667  \n",
       "29  0.166667  \n",
       "22  0.166250  \n",
       "18  0.166250  \n",
       "17  0.165833  \n",
       "1   0.165000  \n",
       "6   0.165000  \n",
       "20  0.164583  \n",
       "4   0.160833  \n",
       "11  0.159583  \n",
       "7   0.159583  \n",
       "5   0.159583  \n",
       "2   0.158750  \n",
       "0   0.157917  \n",
       "9   0.151250  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted = df.sort_values(by='Accuracy', ascending=False)\n",
    "df_sorted.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083bb35",
   "metadata": {},
   "source": [
    "If we look at the table above we can see the performance of various neural networks with different hyperparameters like activation function, hidden layer size and leraning rate. What interesting is that the CNN models do not consistently outperform MLP models in this context. Model 19, an MLP with sigmoid activation and [128, 64] hidden layers, stands out with a little bit higher accuracy. However, trying different activation functions, different hidden layer sizes and different learning rates didnt change the results too much. For some reason both MLP and CNN could not be succesfully trained with the given dataset. I tried many different approaches like using RGB input, changing input sizes, building more complex neural networks but none of them worked. There is only one exception as can be seen from the table which is the CNN with relu activation functions. Among these models a CNN with 1 convolutional layer and [128] hidden layer was the most successful one with an accuracy of 0.333333. But this is still far away from a good optimal model. Despite many efforts and attempts, I still could not achieve good results and unfortunately I still do not know what the reason for this is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
